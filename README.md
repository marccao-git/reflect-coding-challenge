# Reflect Coding Challenge â€“ Setup Guide (Windows & macOS)

Ce projet utilise **PostgreSQL**, **Apache Airflow** et **dbt**.

PostgreSQL sert de base de donnÃ©es centrale pour stocker les donnÃ©es et les tables transformÃ©es.
Apache Airflow permet dâ€™orchestrer et de planifier les pipelines afin de rafraÃ®chir les donnÃ©es automatiquement (toutes les X heures / jours ou Ã  la demande).
dbt est utilisÃ© pour transformer les donnÃ©es et matÃ©rialiser des marts analytiques prÃªts pour lâ€™analyse et le dashboarding.
Lâ€™ensemble permet de disposer de donnÃ©es fiables, Ã  jour et exploitables via une architecture data moderne.

Ce README dÃ©crit **toutes les Ã©tapes nÃ©cessaires aprÃ¨s le clonage du dÃ©pÃ´t**, aussi bien sur **Windows** que sur **macOS**.

---

## ğŸ§° PrÃ©requis

### Commun

* **Git**
* **Docker** & **Docker Compose**
* **Python 3.9.x ou 3.11.x 

---

## ğŸ“¥ 1. Cloner le projet

```bash
git clone https://github.com/marccao-git/reflect-coding-challenge.git
cd reflect-coding-challenge
```

---

## ğŸ 2. CrÃ©er et activer lâ€™environnement virtuel Python

### Windows (PowerShell)

```powershell
python -m venv .venv

# Autoriser lâ€™exÃ©cution des scripts (une seule fois)
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Activer le venv
.\.venv\Scripts\Activate.ps1
```

### macOS / Linux

```bash
python3 -m venv .venv
source .venv/bin/activate
```

VÃ©rification :

```bash
python --version
```

---

## ğŸ“¦ 3. Installer les dÃ©pendances Python

```bash
pip install --upgrade pip
pip install -r requirements.txt
```
---

## 4. Modifier le fichier .env pour la clÃ© API Lucca

Dans le fichier .env, ajouter ou modifier la variable LUCCA_API_KEY avec votre propre clÃ© API Lucca :
LUCCA_API_KEY=VOTRE_CLE_API_ICI
âš ï¸ Cette clÃ© est personnelle, ne la partagez pas publiquement.

---

## ğŸ˜ 5. Base de donnÃ©es PostgreSQL (Docker)

Le projet utilise **une seule instance PostgreSQL** avec **deux bases** :

* `lucca_db` â†’ donnÃ©es applicatives
* `airflow_db` â†’ mÃ©tadonnÃ©es Airflow

### Lancer PostgreSQL + Airflow

```bash
docker compose up -d
```

VÃ©rifier que les conteneurs tournent :

```bash
docker ps
```

---

## ğŸ—„ï¸ 6. CrÃ©er la base Airflow (si nÃ©cessaire)

âš ï¸ Ã€ faire **uniquement si Airflow affiche une erreur `database "airflow_db" does not exist`**

```bash
docker compose exec postgres psql -U lucca_user -d lucca_db -c "CREATE DATABASE airflow_db;"
```

Puis redÃ©marrer Airflow :

```bash
docker compose restart airflow
```

---

## ğŸ‘¤ 7. CrÃ©er lâ€™utilisateur Airflow

### Commande (Windows & macOS)

```bash
docker compose exec airflow airflow users create \
  --username admin \
  --firstname Marc \
  --lastname Marc \
  --role Admin \
  --email marc@example.com \
  --password admin
```

### AccÃ¨s UI

* URL : [http://localhost:8080](http://localhost:8080)
* Login : `admin`
* Mot de passe : `admin`

---

## ğŸ” 8. Lancer dbt

Aller dans le dossier dbt :

```bash
cd dbt
```

### VÃ©rifier dbt

```bash
dbt --version
```

### Lancer les marts

```bash
dbt run
```

ğŸ‘‰ Les **marts** sont matÃ©rialisÃ©s dans PostgreSQL.

---

## ğŸ› ï¸ DÃ©pannage courant

### âŒ dbt crash / erreur Ã©trange

â¡ï¸ VÃ©rifier la version Python

```bash
python --version
```

Doit Ãªtre **3.9.x ou 3.11.x**

---

### âŒ Airflow login impossible

â¡ï¸ VÃ©rifier que lâ€™utilisateur existe :

```bash
docker compose exec airflow airflow users list
```
---

## âœ… RÃ©sumÃ© rapide

1. CrÃ©er le venv
2. Installer les requirements/modifier le .env
3. `docker compose up -d`
4. CrÃ©er `airflow_db` si besoin
5. CrÃ©er lâ€™utilisateur Airflow
6. Se connecter Ã  Airflow 
7. `dbt run`
